
Hello. Myself, Sakthivel Mohan Kumar. I'm doing a master's in data science, and this is my tutorial for machine learning using k NN or k nearest neighbors algorithm, which is actually an algorithm that assumes similar points are likely to have similar labels or values. We're going to go through this pattern where we'll first find out the definition and then the applications of this algorithm, and then we'll go through this IS data set, which is quite popular among the data scientists. And we'll also use codes to find the actual impact of the k value and compare the results.

And finally, we'll compute. Coming to the definition part, k nearest neighbor is a supervised machine learning algorithm used for both classifications and regression tasks. In classification, it predicts the class label of data point based on the majority class of its nearest neighbors. While in regression, it predicts the value of a data point by averaging the values of its nearest neighbors. Coming to the application side, training as practical users in dual fields.

So we'll see some of the examples. Say, for example, movie or product recommendations. Here, we can use the data to find users or items similar to the target user or product and then recommend the items based on their preferences. And we can also use this algorithm in fraud detection. Say, for example, credit card fraud lens detection.

By comparing the new transactions with the previous one, we can classify them as legitimate or fraudulent based on similarities. We can also use it in image recognition where we can identify the label of an image, say, for example, digits or face or animal and so on. Another one of the examples is customer segmentation where we can group customers by purchasing behavior. We can classify new customers into clusters based on their similarity to existing groups by using the kNN algorithm with the data we have. Now we are coming to the Iris dataset, this so popular dataset where we have 3 different varieties of cloud based on the dimensions they have, which are setosa, versicolor, and virginica.

For the iris datasets, we have it contains 150 samples of data points belonging to 3 species. As we said, setosa, versicolor, and virginica. So we are going to consider 2 features for each sample, the settle length and settle width. The goal will be to predict the species of flower based on its features. So we have setosa, versicolor, and virginica plotted on this graph.

The x axis represents the sepal height in centimeters and y axis represents the sepal width in centimeters. The data points are plotted based on the two features of the Iris data set. So the points in yellow represents the features of the setosa data points are yellow while versicolor is represented in green, virginica is represented in blue. Now we have a data point which is highlighted in yellow dot. So this data point needs to be classified in any of these 3 categories.

By using the kNN algorithm, you can have the k value of 3 to see the feature of the nearest 3 data points on the graph to the point we have plotted. So by considering that, we have the most nearest points as virginica in this graph. So we can also use k as 10 to check nearest 10 neighbors and compare the data point that we have in hand. By using k as 10, now we see that there are 7 points which are nearest in virginica and 3 points for versicolor category. But still, we have majority of the data points nearest to the given data point as virginica, while only 3 points all in versicolor category.

So now we can also increase the k value much higher to 20. What happens now is that the given data point is considering the nearest 20 data points where we get 7 virgin eclair lovers, 11 of versicolor, and only 2 of setosa. So now the prediction has a possibility of going wrong because k being so high is considering more data points from versicolor than that of virginica. Though virginica is the nearest, data point, still because of the higher k value, we are considering more points from the versicolor just because they have more data points on the graph. So this might end up in the wrong prediction.

Now we will use this NN algorithm in my Jupyter notebook to analyze the same iris dataset with different k values. Let me take you to my Jupyter notebook. So, initially, we are importing the datasets for setosa, versicolor, and virginica class. And we have the feature names, sepal length, sepal width, petal length, and petal width. Also, we'll introduce the target names because we want to know which datasets fall into which targets or categories.

And then we'll create the data frame with the data points that we have. So we totally have 1 of the data points, which falls into 3 different categories of loss for 50 data points in each category. Now we see that, the target value have 0, 1, and 2, which represents the categories, set us up, versicolor, and virginica. And, also, we'll introduce the column flower name to also mention the name of the category. And when we choose the rows from 45th to 55th, we see that we have both setosa and versicolor, which have 0 and 1 as the values based on their data points differences.

And now we got to split the target values. As I mentioned earlier, first 50 for setosa and next versatile and the last 50 for virginica. Now we are going to visualize these dimensions in the graph. Now we are plotting the sepal length and sepal width. Also, the petal length and petal width.

We are marking one category with blue and the other with green. Now we're going to prepare the data model for the model to predict the data's point, which will be newly introduced, for which we have to split the data into test time training. So I would give him 20% of the data should be for testing while the remaining will be for training. So 120 data points will be for training while the 30 points will be for testing. And we have to initialize the kNN classifier while giving the k value.

I'll leave it to p. And we train the model. We see the efficiency of the model to be 100 percentage. And now we are giving a data point to make a prediction and see which category it has to fall into. So we see that it falls into 0.

Now we'll generate the confusion matrix for the data points that we have. We see that we have all the numbers in diagonal instead of the edges, which means that we have truth and the prediction to be overlapping. So the model works perfectly fine. It means we can see the result here. We have the values 1, which means 100 percentage.

So the model efficiency seems to be 100 percentage. Now we'll go back to change the k value to add 10 instead of 3. So what happens now is that the model will consider the nearest 10 data points instead of 3 and categorize the data point. So we can now see that the model accuracy have changed it from 100 to 96 percentage. And the reason being the prediction and truth has not completely overlapped.

There is one point which is predicted in the virginica category while it has to be worse equal. 1 means worse equal and 2 means worse equal. So predicted is 2 while the truth is 1. So the result reflects the same. I'll show you the POC to be lesser than 100.

So we'll now go back. Now coming back to our computation market, comparing these 2, until we had k value lesser, we got all the predictions to be overlapping with the 2. When we increase the k value, we had a slight deviation in the prediction as we mentioned earlier. So we have analyzed the k value impact in our algorithm using the iris dataset. So as we are concluding it now, the kNN algorithm is a simple yet effective machine learning method for classification and regression.

Now we have used the iris dataset to demonstrate its ability to classify flower species with high accuracy. While k NN is easy to implement and interpret as we just did, it can be computationally intensive for large datasets insensitive to the choice of k. Despite these limitations, it is reliable too for smaller datasets like we used now offering valuable insights with proper tuning and preprocessing. Thus, I would like to conclude that k NN algorithm is a quite useful and interesting machine learning algorithm for smaller datasets and with the right k value. Thank you.